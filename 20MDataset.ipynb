{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "RATINGS_PATH = 'ml-20m/ratings.csv'\n",
    "MOVIE_PATH = 'ml-20m/movies.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .config('spark.driver.maxResultSize', '8g')\\\n",
    "        .config('spark.executor.memory' ,'8g')\\\n",
    "        .config('spark.driver.memory', '8g')\\\n",
    "        .appName('EE551')\\\n",
    "        .getOrCreate()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, DoubleType\n",
    "\n",
    "ratings_schema = StructType([\n",
    "    StructField('userId', IntegerType()),\n",
    "    StructField('itemId', IntegerType()),\n",
    "    StructField('rating', DoubleType()),\n",
    "    StructField('timestamp', IntegerType()),\n",
    "])\n",
    "\n",
    "ratings = spark.read.csv(RATINGS_PATH, \n",
    "                        sep=',',\n",
    "                        header=False,\n",
    "                        schema=ratings_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+----------+\n",
      "|userId|itemId|rating| timestamp|\n",
      "+------+------+------+----------+\n",
      "|     1|     2|   3.5|1112486027|\n",
      "|     1|    29|   3.5|1112484676|\n",
      "|     1|    32|   3.5|1112484819|\n",
      "|     1|    47|   3.5|1112484727|\n",
      "|     1|    50|   3.5|1112484580|\n",
      "|     1|   112|   3.5|1094785740|\n",
      "|     1|   151|   4.0|1094785734|\n",
      "|     1|   223|   4.0|1112485573|\n",
      "|     1|   253|   4.0|1112484940|\n",
      "|     1|   260|   4.0|1112484826|\n",
      "|     1|   293|   4.0|1112484703|\n",
      "|     1|   296|   4.0|1112484767|\n",
      "|     1|   318|   4.0|1112484798|\n",
      "|     1|   337|   3.5|1094785709|\n",
      "|     1|   367|   3.5|1112485980|\n",
      "|     1|   541|   4.0|1112484603|\n",
      "|     1|   589|   3.5|1112485557|\n",
      "|     1|   593|   3.5|1112484661|\n",
      "|     1|   653|   3.0|1094785691|\n",
      "|     1|   919|   3.5|1094785621|\n",
      "+------+------+------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratings = ratings.dropna()\n",
    "\n",
    "ratings.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users = ratings.select('userId').distinct().count()\n",
    "num_movies = ratings.select('itemId').distinct().count()\n",
    "\n",
    "sparsity = (1.0 - ratings.select('rating').count() * 1.0 / (num_users * num_movies))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ratings dataframe is:  99.46001521864456 % empty\n"
     ]
    }
   ],
   "source": [
    "print('The ratings dataframe is: ', sparsity, '% empty')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of movies each user rated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|userId|count|\n",
      "+------+-----+\n",
      "|   148|  128|\n",
      "|   463|   80|\n",
      "|   471|  548|\n",
      "|   496|  168|\n",
      "|   833|   47|\n",
      "|  1088|   60|\n",
      "|  1238|   97|\n",
      "|  1342|   25|\n",
      "|  1580|   42|\n",
      "|  1591|   50|\n",
      "|  1645|  108|\n",
      "|  1829|  288|\n",
      "|  1959|  226|\n",
      "|  2122|  115|\n",
      "|  2142|   29|\n",
      "|  2366|   42|\n",
      "|  2659|  101|\n",
      "|  2866|  940|\n",
      "|  3175|   22|\n",
      "|  3749|   44|\n",
      "+------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratings.groupBy('userId').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create test/train splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train, test) = ratings.randomSplit([0.8, 0.2], seed = 1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build ALS model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "als = ALS(userCol='userId', \n",
    "         itemCol='itemId',\n",
    "         ratingCol='rating',\n",
    "         nonnegative=True,\n",
    "         implicitPrefs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperperparamters to tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = ParamGridBuilder() \\\n",
    "            .addGrid(als.rank, [10, 20]) \\\n",
    "            .addGrid(als.maxIter, [5, 10]) \\\n",
    "            .addGrid(als.regParam, [0.01, 0.05]) \\\n",
    "            .build()\n",
    "\n",
    "evaluator = RegressionEvaluator(metricName='rmse',\n",
    "                               labelCol='rating',\n",
    "                               predictionCol='prediction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CrossValidator(estimator=als, \n",
    "                    estimatorParamMaps=param_grid, \n",
    "                    evaluator=evaluator, \n",
    "                    numFolds=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cv.fit(train)\n",
    "\n",
    "best_model = model.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = best_model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('RMSE: ', evaluator.evaluate(predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
