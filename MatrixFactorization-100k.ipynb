{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN1_DATA_PATH = './ml-100k/ua.base'\n",
    "TEST1_DATA_PATH = './ml-100k/ua.test'\n",
    "TRAIN2_DATA_PATH = './ml-100k/ub.base'\n",
    "TEST2_DATA_PATH = './ml-100k/ub.test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .master('local[*]')\\\n",
    "        .appName('EE551Project')\\\n",
    "        .getOrCreate()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the train and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType\n",
    "\n",
    "train_schema = StructType([\n",
    "    StructField('userId', IntegerType()),\n",
    "    StructField('itemId', IntegerType()),\n",
    "    StructField('rating', IntegerType()),\n",
    "    StructField('timestemp', IntegerType()),\n",
    "])\n",
    "\n",
    "train1 = spark.read.csv(TRAIN1_DATA_PATH,\n",
    "                      sep='\\t',\n",
    "                      header=False,\n",
    "                      schema=train_schema)\n",
    "\n",
    "train2 = spark.read.csv(TRAIN2_DATA_PATH, \n",
    "                        sep='\\t',\n",
    "                        header=False,\n",
    "                        schema=train_schema)\n",
    "\n",
    "train = train1.union(train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- itemId: integer (nullable = true)\n",
      " |-- rating: integer (nullable = true)\n",
      " |-- timestemp: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+---------+\n",
      "|userId|itemId|rating|timestemp|\n",
      "+------+------+------+---------+\n",
      "|     1|     1|     5|874965758|\n",
      "|     1|     2|     3|876893171|\n",
      "|     1|     3|     4|878542960|\n",
      "|     1|     4|     3|876893119|\n",
      "|     1|     5|     3|889751712|\n",
      "+------+------+------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_schema = train_schema \n",
    "\n",
    "test1 = spark.read.csv(TEST1_DATA_PATH,\n",
    "                      sep='\\t',\n",
    "                      header=False,\n",
    "                      schema=test_schema)\n",
    "test2 = spark.read.csv(TEST2_DATA_PATH,\n",
    "                      sep='\\t',\n",
    "                      header=False,\n",
    "                      schema=test_schema)\n",
    "\n",
    "test = test1.union(test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- itemId: integer (nullable = true)\n",
      " |-- rating: integer (nullable = true)\n",
      " |-- timestemp: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+---------+\n",
      "|userId|itemId|rating|timestemp|\n",
      "+------+------+------+---------+\n",
      "|     1|    20|     4|887431883|\n",
      "|     1|    33|     4|878542699|\n",
      "|     1|    61|     4|878542420|\n",
      "|     1|   117|     3|874965739|\n",
      "|     1|   155|     2|878542201|\n",
      "+------+------+------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALS Models\n",
    "## Only using userId, itemId, and rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "\n",
    "als = ALS(userCol='userId', \n",
    "          itemCol='itemId',\n",
    "          ratingCol='rating', \n",
    "          rank=5,\n",
    "          maxIter= 5,\n",
    "          regParam=0.01,\n",
    "          nonnegative = True, \n",
    "          implicitPrefs = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+---------+----------+\n",
      "|userId|itemId|rating|timestemp|prediction|\n",
      "+------+------+------+---------+----------+\n",
      "|   251|   148|     2|886272547| 2.8860848|\n",
      "|   580|   148|     4|884125773| 3.5820909|\n",
      "|   633|   148|     1|875326138| 3.0441895|\n",
      "|   633|   148|     1|875326138| 3.0441895|\n",
      "|   642|   148|     5|885604163| 4.1708684|\n",
      "+------+------+------+---------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = als.fit(train)\n",
    "train_results = model.transform(train)\n",
    "train_results.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE:  0.8201058766532773\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "evaluator = RegressionEvaluator(metricName='rmse', labelCol='rating', predictionCol='prediction')\n",
    "\n",
    "print('RMSE: ', evaluator.evaluate(train_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|            rating|\n",
      "+-------+------------------+\n",
      "|  count|            181140|\n",
      "|   mean| 3.523744065363807|\n",
      "| stddev|1.1258800843524963|\n",
      "|    min|                 1|\n",
      "|    max|                 5|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_results.select('rating').describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+---------+----------+\n",
      "|userId|itemId|rating|timestemp|prediction|\n",
      "+------+------+------+---------+----------+\n",
      "|   251|   148|     2|886272547| 2.8860848|\n",
      "|   580|   148|     4|884125773| 3.5820909|\n",
      "|    27|   148|     3|891543129| 3.0845172|\n",
      "|   332|   148|     5|887938486| 3.8928246|\n",
      "|   602|   148|     4|888638517| 4.0439777|\n",
      "+------+------+------+---------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_predictions = model.transform(test)\n",
    "test_predictions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE:  0.8540376433930686\n"
     ]
    }
   ],
   "source": [
    "print('RMSE: ', evaluator.evaluate(test_predictions))"
   ]
  }
 ],
 "metadata": {
  "gist": {
   "data": {
    "description": "MatrixFactorization-100k.ipynb",
    "public": true
   },
   "id": ""
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
